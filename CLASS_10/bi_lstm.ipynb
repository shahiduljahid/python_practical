{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "864954d8",
   "metadata": {
    "papermill": {
     "duration": 0.0239,
     "end_time": "2023-11-03T11:16:28.729683",
     "exception": false,
     "start_time": "2023-11-03T11:16:28.705783",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> Twitter情感分析（Bi-LSTM） </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18743d78",
   "metadata": {
    "papermill": {
     "duration": 0.023205,
     "end_time": "2023-11-03T11:16:28.822027",
     "exception": false,
     "start_time": "2023-11-03T11:16:28.798822",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"border-radius:10px;border:black solid;padding: 15px;background-color:white;font-size:110%;text-align:left\">\n",
    "<div style=\"font-family:Georgia;background-color:'#DEB887'; padding:30px; font-size:17px\">\n",
    "\n",
    "   \n",
    " \n",
    "<h3 align=\"left\"><font color=purple>📝 项目描述：</font></h3><br> \n",
    "\n",
    "数据集: 该数据集包含74,681条Twitter上的评论，包含4列。<br> \n",
    "\n",
    "1-Index : 索引<br>\n",
    "2-Borderlands : 评论的对象（品牌） <br>\n",
    "3-Mode : 评论者对该文本的情感<br>\n",
    "4-Text : 评论者所分享的文本.<br>\n",
    "\n",
    "注意: 数据集中的原始列名是错误的，应按照上面的来理解。<br>\n",
    "\n",
    "内容: <br>\n",
    "1-加载必要的库<br>\n",
    "2-数据集的概览<br>\n",
    "3-探索性数据分析(Exploratory Data Analysis,EDA)<br>\n",
    "4-文本数据清洗<br>\n",
    "5-NLP中的常见方法:<br>\n",
    "CounterVectorizer <br>\n",
    "TF-IDF <br>\n",
    "N-Grams <br>\n",
    "POS(Part Of Speech)<br>\n",
    "NER(Named Entity Recognition)<br>\n",
    "Stopwords <br>\n",
    "Lemmas and Stems<br>\n",
    "Tokenization<br>\n",
    "显示常用单词和标点符号 <br>\n",
    "Wordcloud可视化<br>\n",
    "6-通过Pytorch和Keras库准备数据集 <br>\n",
    "7-使用双向LSTM对文本进行分类<br>\n",
    "<br> </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9575dd3f",
   "metadata": {
    "papermill": {
     "duration": 0.022901,
     "end_time": "2023-11-03T11:16:28.868059",
     "exception": false,
     "start_time": "2023-11-03T11:16:28.845158",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> 加载库函数</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06382fff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:28.915587Z",
     "iopub.status.busy": "2023-11-03T11:16:28.915225Z",
     "iopub.status.idle": "2023-11-03T11:16:45.604066Z",
     "shell.execute_reply": "2023-11-03T11:16:45.603134Z"
    },
    "papermill": {
     "duration": 16.7156,
     "end_time": "2023-11-03T11:16:45.606548",
     "exception": false,
     "start_time": "2023-11-03T11:16:28.890948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import spacy \n",
    "import warnings\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "\n",
    "\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.tokenize import RegexpTokenizer , TweetTokenizer\n",
    "from nltk.stem import WordNetLemmatizer ,PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from collections import defaultdict\n",
    "from collections import Counter\n",
    "from tensorflow.keras.preprocessing.text import one_hot\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e36c5cbb",
   "metadata": {
    "papermill": {
     "duration": 0.023196,
     "end_time": "2023-11-03T11:16:45.696051",
     "exception": false,
     "start_time": "2023-11-03T11:16:45.672855",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 首先，我们加载数据集，并显示其中的7个随机样本<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0d0a51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:45.745001Z",
     "iopub.status.busy": "2023-11-03T11:16:45.743674Z",
     "iopub.status.idle": "2023-11-03T11:16:46.078345Z",
     "shell.execute_reply": "2023-11-03T11:16:46.077468Z"
    },
    "papermill": {
     "duration": 0.361476,
     "end_time": "2023-11-03T11:16:46.080741",
     "exception": false,
     "start_time": "2023-11-03T11:16:45.719265",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./twitter_training.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cb2431",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.130210Z",
     "iopub.status.busy": "2023-11-03T11:16:46.129356Z",
     "iopub.status.idle": "2023-11-03T11:16:46.150690Z",
     "shell.execute_reply": "2023-11-03T11:16:46.149823Z"
    },
    "papermill": {
     "duration": 0.047769,
     "end_time": "2023-11-03T11:16:46.152563",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.104794",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88385d7",
   "metadata": {
    "papermill": {
     "duration": 0.022993,
     "end_time": "2023-11-03T11:16:46.198958",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.175965",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 查看一下数据集的形状，列名称，以及数据类型<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a101788d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.246862Z",
     "iopub.status.busy": "2023-11-03T11:16:46.246550Z",
     "iopub.status.idle": "2023-11-03T11:16:46.251160Z",
     "shell.execute_reply": "2023-11-03T11:16:46.250285Z"
    },
    "papermill": {
     "duration": 0.031356,
     "end_time": "2023-11-03T11:16:46.253570",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.222214",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The shape of the dataset is : {df.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54616a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.301640Z",
     "iopub.status.busy": "2023-11-03T11:16:46.301370Z",
     "iopub.status.idle": "2023-11-03T11:16:46.306166Z",
     "shell.execute_reply": "2023-11-03T11:16:46.305311Z"
    },
    "papermill": {
     "duration": 0.031236,
     "end_time": "2023-11-03T11:16:46.308319",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.277083",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The columns are :{df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1416e50",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.356414Z",
     "iopub.status.busy": "2023-11-03T11:16:46.356084Z",
     "iopub.status.idle": "2023-11-03T11:16:46.361982Z",
     "shell.execute_reply": "2023-11-03T11:16:46.360928Z"
    },
    "papermill": {
     "duration": 0.0323,
     "end_time": "2023-11-03T11:16:46.363950",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.331650",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The dtypes of the dataset : \\n\\n{df.dtypes}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d429b2d",
   "metadata": {
    "papermill": {
     "duration": 0.023824,
     "end_time": "2023-11-03T11:16:46.411556",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.387732",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 查看一下数据集中的空(null)数据及其占比，重复数据及其占比 <br>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af47ab72",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.460276Z",
     "iopub.status.busy": "2023-11-03T11:16:46.459460Z",
     "iopub.status.idle": "2023-11-03T11:16:46.571011Z",
     "shell.execute_reply": "2023-11-03T11:16:46.570102Z"
    },
    "papermill": {
     "duration": 0.138229,
     "end_time": "2023-11-03T11:16:46.573209",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.434980",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.describe(include='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "836ede28",
   "metadata": {
    "papermill": {
     "duration": 0.023717,
     "end_time": "2023-11-03T11:16:46.621041",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.597324",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> 探索性数据分析 (Exploratory Data Analysis, EDA)</b></div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f918e62",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.670251Z",
     "iopub.status.busy": "2023-11-03T11:16:46.669895Z",
     "iopub.status.idle": "2023-11-03T11:16:46.675693Z",
     "shell.execute_reply": "2023-11-03T11:16:46.674852Z"
    },
    "papermill": {
     "duration": 0.032849,
     "end_time": "2023-11-03T11:16:46.677540",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.644691",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def show_details(dataset):\n",
    "    missed_values = dataset.isnull().sum()\n",
    "    missed_values_percent = (dataset.isnull().sum()) / len(dataset)\n",
    "    duplicated_values = dataset.duplicated().sum()\n",
    "    duplicated_values_percent = (dataset.duplicated().sum()) / len(dataset)\n",
    "    info_frame = pd.DataFrame({'Missed_Values' : missed_values , \n",
    "                              'Missed_Values %' :missed_values_percent,\n",
    "                              'Duplicated values' :duplicated_values,\n",
    "                              'Duplicated values %':duplicated_values_percent})\n",
    "    return info_frame.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9b925a",
   "metadata": {
    "papermill": {
     "duration": 0.023617,
     "end_time": "2023-11-03T11:16:46.724808",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.701191",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 我们可以看到，少数数据有遗漏和重复的现象，这些数据需要被丢弃。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0655dd13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.773476Z",
     "iopub.status.busy": "2023-11-03T11:16:46.773204Z",
     "iopub.status.idle": "2023-11-03T11:16:46.900575Z",
     "shell.execute_reply": "2023-11-03T11:16:46.899645Z"
    },
    "papermill": {
     "duration": 0.154018,
     "end_time": "2023-11-03T11:16:46.902586",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.748568",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "show_details(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd2c5361",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:46.952974Z",
     "iopub.status.busy": "2023-11-03T11:16:46.952650Z",
     "iopub.status.idle": "2023-11-03T11:16:47.151577Z",
     "shell.execute_reply": "2023-11-03T11:16:47.150652Z"
    },
    "papermill": {
     "duration": 0.226118,
     "end_time": "2023-11-03T11:16:47.153647",
     "exception": false,
     "start_time": "2023-11-03T11:16:46.927529",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.drop_duplicates(inplace=True)\n",
    "df.dropna(inplace=True)\n",
    "show_details(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8813c518",
   "metadata": {
    "papermill": {
     "duration": 0.024315,
     "end_time": "2023-11-03T11:16:47.202651",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.178336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 在这里，我们将列的名称更改为恰当的名称:<br>\n",
    "'2401' : 'Index' , 'Borderlands': 'Land' , 'Positive' : 'Mode' , \"im getting on borderlands and i will murder you all ,\": 'Text'\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30175f41",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:47.253131Z",
     "iopub.status.busy": "2023-11-03T11:16:47.252420Z",
     "iopub.status.idle": "2023-11-03T11:16:47.257550Z",
     "shell.execute_reply": "2023-11-03T11:16:47.256694Z"
    },
    "papermill": {
     "duration": 0.032481,
     "end_time": "2023-11-03T11:16:47.259464",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.226983",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.rename(columns={'2401' : 'Index' , 'Borderlands': 'Land' , 'Positive' : 'Mode' \n",
    "                   , \"im getting on borderlands and i will murder you all ,\": 'Text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f2c620",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:47.312483Z",
     "iopub.status.busy": "2023-11-03T11:16:47.311772Z",
     "iopub.status.idle": "2023-11-03T11:16:47.318243Z",
     "shell.execute_reply": "2023-11-03T11:16:47.317342Z"
    },
    "papermill": {
     "duration": 0.035959,
     "end_time": "2023-11-03T11:16:47.320290",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.284331",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eec21f9e",
   "metadata": {
    "papermill": {
     "duration": 0.025191,
     "end_time": "2023-11-03T11:16:47.371575",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.346384",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 这里，我们查看一下Lands这一列，发现有32个不同的名称，并用dataframe和柱状图显示每个名称的评论数量。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "863f1e0a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:47.423243Z",
     "iopub.status.busy": "2023-11-03T11:16:47.422407Z",
     "iopub.status.idle": "2023-11-03T11:16:47.440738Z",
     "shell.execute_reply": "2023-11-03T11:16:47.439855Z"
    },
    "papermill": {
     "duration": 0.046186,
     "end_time": "2023-11-03T11:16:47.442586",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.396400",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The number of unique lands : {len(df.Land.unique())}')\n",
    "print('**' * 40)\n",
    "df.Land.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53e61b29",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:47.493489Z",
     "iopub.status.busy": "2023-11-03T11:16:47.493214Z",
     "iopub.status.idle": "2023-11-03T11:16:47.511658Z",
     "shell.execute_reply": "2023-11-03T11:16:47.510776Z"
    },
    "papermill": {
     "duration": 0.046001,
     "end_time": "2023-11-03T11:16:47.513610",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.467609",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "lands =df.Land.value_counts()\n",
    "lands.to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95c45293",
   "metadata": {
    "papermill": {
     "duration": 0.024993,
     "end_time": "2023-11-03T11:16:47.565705",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.540712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 可以看到，不同对象的评论数的分布是较为均匀的，从2150条到2328条。<br>\n",
    "* 这里我们显示评论数最多的10个对象 \n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aab20cb1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:47.616736Z",
     "iopub.status.busy": "2023-11-03T11:16:47.616428Z",
     "iopub.status.idle": "2023-11-03T11:16:48.051950Z",
     "shell.execute_reply": "2023-11-03T11:16:48.051077Z"
    },
    "papermill": {
     "duration": 0.464011,
     "end_time": "2023-11-03T11:16:48.054581",
     "exception": false,
     "start_time": "2023-11-03T11:16:47.590570",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sns.set_style('darkgrid')\n",
    "plt.figure(figsize=(10,6))\n",
    "bar = sns.barplot(x=lands.values[:10] ,y=lands.index[:10] , palette='rocket')\n",
    "bar.bar_label(bar.containers[0])\n",
    "plt.title('Top 10 Lands')\n",
    "plt.xlabel('Count')\n",
    "plt.ylabel('Land')\n",
    "plt.xlim(0 , 2500)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "187c05d4",
   "metadata": {
    "papermill": {
     "duration": 0.025458,
     "end_time": "2023-11-03T11:16:48.108389",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.082931",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* Mode这一列包含4种：Positive，Neutral，Negative和Irrelevant<br>\n",
    "Negative的评价数量最多: 21698 <br>\n",
    "Positive的排第二: 19712<br>\n",
    "Neutral的排第三: 17708<br>\n",
    "Irrelevant的最少: 12537<br>\n",
    "* 用一个饼图来直观地显示一下所占的比例\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e98b36a9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.161933Z",
     "iopub.status.busy": "2023-11-03T11:16:48.161270Z",
     "iopub.status.idle": "2023-11-03T11:16:48.178236Z",
     "shell.execute_reply": "2023-11-03T11:16:48.177048Z"
    },
    "papermill": {
     "duration": 0.046219,
     "end_time": "2023-11-03T11:16:48.180171",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.133952",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f'The unique values of Mode : {len(df.Mode.unique())}')\n",
    "print('**' * 20)\n",
    "print(df.Mode.unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a1d54c0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.232748Z",
     "iopub.status.busy": "2023-11-03T11:16:48.232478Z",
     "iopub.status.idle": "2023-11-03T11:16:48.249767Z",
     "shell.execute_reply": "2023-11-03T11:16:48.248918Z"
    },
    "papermill": {
     "duration": 0.045625,
     "end_time": "2023-11-03T11:16:48.251662",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.206037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "mode = df.Mode.value_counts()\n",
    "mode.to_frame().T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aef4015",
   "metadata": {
    "papermill": {
     "duration": 0.025784,
     "end_time": "2023-11-03T11:16:48.303406",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.277622",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "\n",
    "Negative : 30.3 % <br>\n",
    "Positive : 27.5%<br>\n",
    "Neutral : 24.7%<br>\n",
    "Irrelevant : 17.5%<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0766b457",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.356575Z",
     "iopub.status.busy": "2023-11-03T11:16:48.356240Z",
     "iopub.status.idle": "2023-11-03T11:16:48.494831Z",
     "shell.execute_reply": "2023-11-03T11:16:48.493876Z"
    },
    "papermill": {
     "duration": 0.16959,
     "end_time": "2023-11-03T11:16:48.499015",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.329425",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,6))\n",
    "plt.pie(x = mode.values , labels=mode.keys() ,autopct=\"%1.1f%%\" , \n",
    "textprops={\"fontsize\":10,\"fontweight\":\"black\"},colors=sns.color_palette(\"rocket\"))\n",
    "plt.title('Mode Distribution') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343f92a7",
   "metadata": {
    "papermill": {
     "duration": 0.042848,
     "end_time": "2023-11-03T11:16:48.599062",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.556214",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 我们还可以分析一下每个评论对象下的情感的分布：<br>\n",
    "\n",
    "Irrelevant : Battlefield最多，有907条；TomClancysGhostRecon最少，只有92条<br>\n",
    "\n",
    "Negative :   MaddenNFL最多，有1665条；RedDeadRedemption(RDR)最少，只有290条<br>\n",
    "\n",
    "Neutral :    Amazon最多，有1197条，AssassinsCreed最少，只有153条<br>\n",
    "\n",
    "Positive :   AssassinsCreed最多，有1382条；Facebook最少，只有154条<br>\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b3a9de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.653963Z",
     "iopub.status.busy": "2023-11-03T11:16:48.653105Z",
     "iopub.status.idle": "2023-11-03T11:16:48.751579Z",
     "shell.execute_reply": "2023-11-03T11:16:48.750663Z"
    },
    "papermill": {
     "duration": 0.128072,
     "end_time": "2023-11-03T11:16:48.753590",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.625518",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.crosstab(df.Mode , df.Land).T.style.background_gradient( subset=['Negative'],cmap='Reds')\\\n",
    ".background_gradient(subset=['Positive'] , cmap='Greens')\\\n",
    ".background_gradient(subset=['Irrelevant'] , cmap='BuGn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18355218",
   "metadata": {
    "papermill": {
     "duration": 0.027511,
     "end_time": "2023-11-03T11:16:48.808862",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.781351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 在这里，利用re这个库，我们可以替换一些文本中的语法问题、口语化表达等... <br>\n",
    "* 还可以替换或者删除推文中emoji表情，这些emoji表情对于判断人们的情感倾向至关重要 <br>\n",
    "* 使用text_clear函数将连续的多个标点替换为1个标点<br>\n",
    "* 最后，所有的文本都转化为小写形式<br> \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fd5178c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.865683Z",
     "iopub.status.busy": "2023-11-03T11:16:48.864987Z",
     "iopub.status.idle": "2023-11-03T11:16:48.877006Z",
     "shell.execute_reply": "2023-11-03T11:16:48.876155Z"
    },
    "papermill": {
     "duration": 0.04262,
     "end_time": "2023-11-03T11:16:48.878884",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.836264",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def clean_emoji(tx):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols \n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport \n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags \n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    \n",
    "    return emoji_pattern.sub(r'', tx)\n",
    "def text_cleaner(tx):\n",
    "    \n",
    "    text = re.sub(r\"won\\'t\", \"would not\", tx)\n",
    "    text = re.sub(r\"im\", \"i am\", text)\n",
    "    text = re.sub(r\"Im\", \"I am\", text)\n",
    "    text = re.sub(r\"can\\'t\", \"can not\", text)\n",
    "    text = re.sub(r\"don\\'t\", \"do not\", text)\n",
    "    text = re.sub(r\"shouldn\\'t\", \"should not\", text)\n",
    "    text = re.sub(r\"needn\\'t\", \"need not\", text)\n",
    "    text = re.sub(r\"hasn\\'t\", \"has not\", text)\n",
    "    text = re.sub(r\"haven\\'t\", \"have not\", text)\n",
    "    text = re.sub(r\"weren\\'t\", \"were not\", text)\n",
    "    text = re.sub(r\"mightn\\'t\", \"might not\", text)\n",
    "    text = re.sub(r\"didn\\'t\", \"did not\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'s\", \" is\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\"\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub('https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z0-9\\!\\?\\.\\@]',' ' , text)\n",
    "    text = re.sub(r'[!]+' , '!' , text)\n",
    "    text = re.sub(r'[?]+' , '?' , text)\n",
    "    text = re.sub(r'[.]+' , '.' , text)\n",
    "    text = re.sub(r'[@]+' , '@' , text)\n",
    "    text = re.sub(r'unk' , ' ' , text)\n",
    "    text = re.sub('\\n', '', text)\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[ ]+' , ' ' , text)\n",
    "    \n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f70499",
   "metadata": {
    "papermill": {
     "duration": 0.027234,
     "end_time": "2023-11-03T11:16:48.933506",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.906272",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "* 这里介绍几个NLP领域常见的任务<br>\n",
    "首先通过random.choice选择一个样本<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1349c484",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:48.992501Z",
     "iopub.status.busy": "2023-11-03T11:16:48.991509Z",
     "iopub.status.idle": "2023-11-03T11:16:49.005434Z",
     "shell.execute_reply": "2023-11-03T11:16:49.004556Z"
    },
    "papermill": {
     "duration": 0.046137,
     "end_time": "2023-11-03T11:16:49.007332",
     "exception": false,
     "start_time": "2023-11-03T11:16:48.961195",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "random.seed(99)\n",
    "test_text =text_cleaner( random.choice(df['Text']))\n",
    "test_text = clean_emoji(test_text)\n",
    "test_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bec94928",
   "metadata": {
    "papermill": {
     "duration": 0.028601,
     "end_time": "2023-11-03T11:16:49.064693",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.036092",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "POS => 词性（Part Of Speech）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c568c4e0",
   "metadata": {
    "papermill": {
     "duration": 0.02715,
     "end_time": "2023-11-03T11:16:49.119866",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.092716",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**POS:**<br>\n",
    "词性（Part Of Speech，POS）表示单词在句子中的意义和语法功能<br>\n",
    "英语中有8种词性 (noun, pronoun, verb, adjective, adverb, preposition, conjunction, and interjection)<br>\n",
    "在使用词典时，理解词性对于确定单词的正确定义至关重要。\n",
    "\n",
    "**Description**<br>\n",
    "这里使用Spacy库中的特征来判断词性(nlp = spacy.load(\"en_core_web_sm\"))<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d99c5bfd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:49.176262Z",
     "iopub.status.busy": "2023-11-03T11:16:49.175885Z",
     "iopub.status.idle": "2023-11-03T11:16:49.202751Z",
     "shell.execute_reply": "2023-11-03T11:16:49.201751Z"
    },
    "papermill": {
     "duration": 0.057503,
     "end_time": "2023-11-03T11:16:49.204751",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.147248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for token in doc :\n",
    "    print(f'{token} => {token.pos_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9608475",
   "metadata": {
    "papermill": {
     "duration": 0.027385,
     "end_time": "2023-11-03T11:16:49.259520",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.232135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "命名实体识别 (Named Entity Recognition)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78311b39",
   "metadata": {
    "papermill": {
     "duration": 0.027194,
     "end_time": "2023-11-03T11:16:49.314379",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.287185",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**NER:**<br>\n",
    "NER表示命名实体识别，其中，命名实体是表示真实世界对象的特定术语，如人员、组织、位置和日期等。NER用于识别、分类和提取（文本中的命名实体）非结构化文本中最重要的信息。它对于从大量数据中快速提取关键信息特别有用，因为它可以自动提取重要信息。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ab8727",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:49.370504Z",
     "iopub.status.busy": "2023-11-03T11:16:49.370167Z",
     "iopub.status.idle": "2023-11-03T11:16:49.385890Z",
     "shell.execute_reply": "2023-11-03T11:16:49.384880Z"
    },
    "papermill": {
     "duration": 0.046109,
     "end_time": "2023-11-03T11:16:49.387777",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.341668",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for chunk in doc.ents:\n",
    "    print(f'{chunk} => {chunk.label_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8702ab3d",
   "metadata": {
    "papermill": {
     "duration": 0.027148,
     "end_time": "2023-11-03T11:16:49.442499",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.415351",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Chunking:**<br>\n",
    "将文本转换为较小的单词以更易于理解，或者根据单词的性质将相似的单词组合在一起的过程。\n",
    "如名词组，动词组等。 <br>\n",
    "\n",
    "NP: Noun Chunks<br>\n",
    "VP: Verp Chunks\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0e4149",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:49.499951Z",
     "iopub.status.busy": "2023-11-03T11:16:49.499354Z",
     "iopub.status.idle": "2023-11-03T11:16:49.514119Z",
     "shell.execute_reply": "2023-11-03T11:16:49.513007Z"
    },
    "papermill": {
     "duration": 0.045531,
     "end_time": "2023-11-03T11:16:49.516306",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.470775",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "doc = nlp(test_text)\n",
    "for chunk in doc.noun_chunks:\n",
    "    print(f'{chunk} => {chunk.label_}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8fcfe07",
   "metadata": {
    "papermill": {
     "duration": 0.027617,
     "end_time": "2023-11-03T11:16:49.573172",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.545555",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Tokenization :**<br>\n",
    "分词（Tokenization）将文本分解成更小的部分，以便于机器分析，帮助机器理解人类语言。<br>\n",
    "这些更小的部分被称作token。<br>\n",
    "\n",
    "注意: <br>\n",
    "有各种类型的分词，如：RegexpTokenizaton、TweetTokenization等。<br>\n",
    "每种分词都有不同的方法将文本分解为token。<br>\n",
    "\n",
    "**Description**<br>\n",
    "这里我们使用Regexp分词，它使用正则表达式将字符串拆分为子字符串。<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e984564a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:49.630705Z",
     "iopub.status.busy": "2023-11-03T11:16:49.629944Z",
     "iopub.status.idle": "2023-11-03T11:16:49.636906Z",
     "shell.execute_reply": "2023-11-03T11:16:49.636030Z"
    },
    "papermill": {
     "duration": 0.037906,
     "end_time": "2023-11-03T11:16:49.638854",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.600948",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tokenizer = TweetTokenizer()\n",
    "Tokenizer=RegexpTokenizer(r'\\w+')\n",
    "test_text_tokenized = Tokenizer.tokenize(test_text)\n",
    "test_text_tokenized\n",
    "\n",
    "# df['Text']=df['Text'].apply(lambda x : Tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ca568f",
   "metadata": {
    "papermill": {
     "duration": 0.027715,
     "end_time": "2023-11-03T11:16:49.694794",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.667079",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    " Counter vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0531c818",
   "metadata": {
    "papermill": {
     "duration": 0.027569,
     "end_time": "2023-11-03T11:16:49.750169",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.722600",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**CountVectorizer:**<br>\n",
    "Counter Vectorizer用于将文档、文本转换为分词(token)出现频次的矢量。<br>例如，在下面的句子中，我们可以看到is已经出现了2次，而其他单词只出现了一次。<br>因此，会有一个向量来描述每个单词在句子中出现的次数。为了更好地理解，这里用一个热力图来显示。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c37f5f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:49.808013Z",
     "iopub.status.busy": "2023-11-03T11:16:49.807304Z",
     "iopub.status.idle": "2023-11-03T11:16:50.258240Z",
     "shell.execute_reply": "2023-11-03T11:16:50.257318Z"
    },
    "papermill": {
     "duration": 0.482268,
     "end_time": "2023-11-03T11:16:50.260286",
     "exception": false,
     "start_time": "2023-11-03T11:16:49.778018",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n",
    "counter_vectorizer = CountVectorizer()\n",
    "transform = counter_vectorizer.fit_transform([test_text]).toarray()\n",
    "sns.heatmap(transform, annot=True,xticklabels=words, \n",
    "        cbar=False)\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "044f79d3",
   "metadata": {
    "papermill": {
     "duration": 0.028777,
     "end_time": "2023-11-03T11:16:50.317996",
     "exception": false,
     "start_time": "2023-11-03T11:16:50.289219",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**TF-IDF:**<br>\n",
    "Tf_IDF：Tf-IDF由两部分组成。首先，Tf代表term frequency，表示一个单词在文档中出现的次数<br>（计算单词的数量并将其除以句子中所有单词的数量）<br>\n",
    "\n",
    "IDF：代表反向文档频率，是指一个单词在语料库中的常见程度或一个单词的不常见程度<br>（衡量术语在语料库中所有文档中的重要性）<br>结果实际上是一个介于0和1之间的数字，它是通过将语料库中文档总数的对数除以该术语出现的文档数来计算的\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12710703",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:50.376973Z",
     "iopub.status.busy": "2023-11-03T11:16:50.376270Z",
     "iopub.status.idle": "2023-11-03T11:16:50.827100Z",
     "shell.execute_reply": "2023-11-03T11:16:50.826202Z"
    },
    "papermill": {
     "duration": 0.482653,
     "end_time": "2023-11-03T11:16:50.829250",
     "exception": false,
     "start_time": "2023-11-03T11:16:50.346597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "words  = ['ghost','of','tsushima','now','graphically','is','best','open','world','red','dead','redemption','2','one','second','ahead']\n",
    "TF_IDF = TfidfVectorizer()\n",
    "transform = TF_IDF.fit_transform([test_text]).toarray()\n",
    "sns.heatmap(transform, annot=True,xticklabels=words, \n",
    "        cbar=False)\n",
    "transform"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1275aa5f",
   "metadata": {
    "papermill": {
     "duration": 0.030834,
     "end_time": "2023-11-03T11:16:50.889838",
     "exception": false,
     "start_time": "2023-11-03T11:16:50.859004",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**N-grams:**<br>\n",
    "N-gram表示N个单词的序列。文本文档中N个连续项目的集合，包括单词、数字、符号、<br>和标点符号。如果是2-gram，那么就是基于当前单词和下一个单词。3-gram，当前和后面的2个单词，以此类推。<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f876282",
   "metadata": {
    "papermill": {
     "duration": 0.02904,
     "end_time": "2023-11-03T11:16:50.947925",
     "exception": false,
     "start_time": "2023-11-03T11:16:50.918885",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "N-grams => 3-grams "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f695d40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:51.007643Z",
     "iopub.status.busy": "2023-11-03T11:16:51.006653Z",
     "iopub.status.idle": "2023-11-03T11:16:51.014957Z",
     "shell.execute_reply": "2023-11-03T11:16:51.014131Z"
    },
    "papermill": {
     "duration": 0.040201,
     "end_time": "2023-11-03T11:16:51.016864",
     "exception": false,
     "start_time": "2023-11-03T11:16:50.976663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def n_grams(text, n):\n",
    "\n",
    "    return [text[i:i+n] for i in range(len(text)-n+1)]\n",
    "cleaned = test_text_tokenized\n",
    "n_grams(cleaned, 3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80125d00",
   "metadata": {
    "papermill": {
     "duration": 0.028804,
     "end_time": "2023-11-03T11:16:51.074504",
     "exception": false,
     "start_time": "2023-11-03T11:16:51.045700",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Stop words:**<br>\n",
    "Stop words表示语言中非常常见的单词。在许多句子中，即使它们被删除也不会影响句子的意思<br>\n",
    "\n",
    "NLTK库包含了NLP项目中常用的stop words。<br> 例如，英语中有179个stop words，这里展示了其中的20个。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfbe6c51",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:51.133804Z",
     "iopub.status.busy": "2023-11-03T11:16:51.132997Z",
     "iopub.status.idle": "2023-11-03T11:16:51.143367Z",
     "shell.execute_reply": "2023-11-03T11:16:51.142313Z"
    },
    "papermill": {
     "duration": 0.042079,
     "end_time": "2023-11-03T11:16:51.145246",
     "exception": false,
     "start_time": "2023-11-03T11:16:51.103167",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "stopwords_list = stopwords.words('english')\n",
    "print(f'There are {len(stopwords_list) } stop words')\n",
    "print('**' * 20 , '\\n20 of them are as follows:\\n')\n",
    "for inx , value in enumerate(stopwords_list[:20]):\n",
    "    print(f'{inx+1}:{value}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9a305d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:51.264269Z",
     "iopub.status.busy": "2023-11-03T11:16:51.263400Z",
     "iopub.status.idle": "2023-11-03T11:16:51.268832Z",
     "shell.execute_reply": "2023-11-03T11:16:51.267899Z"
    },
    "papermill": {
     "duration": 0.038464,
     "end_time": "2023-11-03T11:16:51.270915",
     "exception": false,
     "start_time": "2023-11-03T11:16:51.232451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_corpus(kind):\n",
    "    corpus = []\n",
    "    for text in df.loc[df['Mode']==kind]['Text'].str.split():\n",
    "        for word in text:\n",
    "            corpus.append(word)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb6f77cd",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:51.330741Z",
     "iopub.status.busy": "2023-11-03T11:16:51.330411Z",
     "iopub.status.idle": "2023-11-03T11:16:56.001287Z",
     "shell.execute_reply": "2023-11-03T11:16:56.000402Z"
    },
    "papermill": {
     "duration": 4.703759,
     "end_time": "2023-11-03T11:16:56.003920",
     "exception": false,
     "start_time": "2023-11-03T11:16:51.300161",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "stop = stopwords.words('english')\n",
    "sentiments = list(df.Mode.unique())\n",
    "\n",
    "\n",
    "\n",
    "for inx , value in enumerate(sentiments):\n",
    "    \n",
    "    corpus = make_corpus(value)\n",
    "    \n",
    "    dic = defaultdict(int)\n",
    "\n",
    "    for word in corpus:\n",
    "        if word in stop:\n",
    "            dic[word] += 1\n",
    "    \n",
    "    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n",
    "\n",
    "    x, y = zip(*top)\n",
    "    plt.title(f'{value} ')\n",
    "    plt.bar(x , y)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a5fa567",
   "metadata": {
    "papermill": {
     "duration": 0.031572,
     "end_time": "2023-11-03T11:16:56.066593",
     "exception": false,
     "start_time": "2023-11-03T11:16:56.035021",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "分析完数据，开始清理数据。这里用text_clearer清洗数据，然后用Tokenizer来进行分词。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f36d13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:16:56.132952Z",
     "iopub.status.busy": "2023-11-03T11:16:56.132296Z",
     "iopub.status.idle": "2023-11-03T11:17:01.811248Z",
     "shell.execute_reply": "2023-11-03T11:17:01.810207Z"
    },
    "papermill": {
     "duration": 5.714029,
     "end_time": "2023-11-03T11:17:01.813482",
     "exception": false,
     "start_time": "2023-11-03T11:16:56.099453",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df['Text'] = df['Text'].apply(lambda x : clean_emoji(x))\n",
    "df['Text'] = df['Text'].apply(lambda x : text_cleaner(x))\n",
    "df['Text']= df['Text'].apply(lambda x : Tokenizer.tokenize(x))\n",
    "df['Text'].to_frame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0705203d",
   "metadata": {
    "papermill": {
     "duration": 0.030651,
     "end_time": "2023-11-03T11:17:01.875714",
     "exception": false,
     "start_time": "2023-11-03T11:17:01.845063",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "词根（Lemmas）和词干（Stems）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d3cd699",
   "metadata": {
    "papermill": {
     "duration": 0.030072,
     "end_time": "2023-11-03T11:17:01.936408",
     "exception": false,
     "start_time": "2023-11-03T11:17:01.906336",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Lemmatization :**<br>\n",
    "\n",
    "Lemmatization可以将单词简化为词根。如动词fly，它可以有许多不同的形式，flow，flew，flies，flown，flowing等。<br>这些都可以简化为fly。<br>\n",
    "\n",
    "**Stemming :**<br>\n",
    "Stemming表示利用人为定义的规则来删除单词的结尾，以将它们缩减为词干的形式。 <br>\n",
    "\n",
    "**二者的不同点 :**<br>\n",
    "Lemmatization和Stemming之间最重要的区别是，Lemmatization更准确，它将一个单词变成该单词的语言词根，Stemming可以是任何对计算机可读但是对人类不可读（有时不可读）的东西。通常Stem生成也更快，是处理大量语料库的好方法。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f6d13ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:01.999754Z",
     "iopub.status.busy": "2023-11-03T11:17:01.998915Z",
     "iopub.status.idle": "2023-11-03T11:17:03.014152Z",
     "shell.execute_reply": "2023-11-03T11:17:03.012941Z"
    },
    "papermill": {
     "duration": 1.049019,
     "end_time": "2023-11-03T11:17:03.016271",
     "exception": false,
     "start_time": "2023-11-03T11:17:01.967252",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(test_text)\n",
    "for token in doc :\n",
    "    print(f'{token} => {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd413d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:03.079293Z",
     "iopub.status.busy": "2023-11-03T11:17:03.078973Z",
     "iopub.status.idle": "2023-11-03T11:17:31.813911Z",
     "shell.execute_reply": "2023-11-03T11:17:31.813110Z"
    },
    "papermill": {
     "duration": 28.768626,
     "end_time": "2023-11-03T11:17:31.816220",
     "exception": false,
     "start_time": "2023-11-03T11:17:03.047594",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# lemmatizer = WordNetLemmatizer() \n",
    "Stemmer = PorterStemmer()\n",
    "def stopwords_cleaner(text):\n",
    "#     word = [lemmatizer.lemmatize(letter) for letter in text if letter not in stopwords_list]\n",
    "    word = [Stemmer.stem(letter) for letter in text if letter not in stopwords_list]\n",
    "    peasting = ' '.join(word)\n",
    "    return peasting\n",
    "df['Text'] = df['Text'].apply(lambda x : stopwords_cleaner(x))\n",
    "# stopwords_cleaner(Tokenizer.tokenize(df.Text[100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81fa1e0c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:31.879944Z",
     "iopub.status.busy": "2023-11-03T11:17:31.879665Z",
     "iopub.status.idle": "2023-11-03T11:17:31.889021Z",
     "shell.execute_reply": "2023-11-03T11:17:31.888133Z"
    },
    "papermill": {
     "duration": 0.042906,
     "end_time": "2023-11-03T11:17:31.890893",
     "exception": false,
     "start_time": "2023-11-03T11:17:31.847987",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Text'][:10].to_frame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26aa3629",
   "metadata": {
    "papermill": {
     "duration": 0.031073,
     "end_time": "2023-11-03T11:17:31.952846",
     "exception": false,
     "start_time": "2023-11-03T11:17:31.921773",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**WordCloud**<br>\n",
    "WordCloud是一种用于表示文本数据的可视化技术，其中每个单词的大小表示其频率或重要性。<br>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a04ec08",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:32.017306Z",
     "iopub.status.busy": "2023-11-03T11:17:32.016930Z",
     "iopub.status.idle": "2023-11-03T11:17:35.503992Z",
     "shell.execute_reply": "2023-11-03T11:17:35.503089Z"
    },
    "papermill": {
     "duration": 3.524995,
     "end_time": "2023-11-03T11:17:35.508966",
     "exception": false,
     "start_time": "2023-11-03T11:17:31.983971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Positive']['Text']\n",
    "pos = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(pos)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Positive Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed84069e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:35.585702Z",
     "iopub.status.busy": "2023-11-03T11:17:35.584973Z",
     "iopub.status.idle": "2023-11-03T11:17:39.134287Z",
     "shell.execute_reply": "2023-11-03T11:17:39.133384Z"
    },
    "papermill": {
     "duration": 3.592244,
     "end_time": "2023-11-03T11:17:39.139310",
     "exception": false,
     "start_time": "2023-11-03T11:17:35.547066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Negative']['Text']\n",
    "neg = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(neg)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Negative Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78b695d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:39.225261Z",
     "iopub.status.busy": "2023-11-03T11:17:39.224909Z",
     "iopub.status.idle": "2023-11-03T11:17:42.712726Z",
     "shell.execute_reply": "2023-11-03T11:17:42.711781Z"
    },
    "papermill": {
     "duration": 3.536204,
     "end_time": "2023-11-03T11:17:42.718298",
     "exception": false,
     "start_time": "2023-11-03T11:17:39.182094",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Neutral']['Text']\n",
    "Neutral = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(Neutral)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Neutral Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e287c8ea",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:42.819875Z",
     "iopub.status.busy": "2023-11-03T11:17:42.819539Z",
     "iopub.status.idle": "2023-11-03T11:17:45.938593Z",
     "shell.execute_reply": "2023-11-03T11:17:45.937735Z"
    },
    "papermill": {
     "duration": 3.175463,
     "end_time": "2023-11-03T11:17:45.943833",
     "exception": false,
     "start_time": "2023-11-03T11:17:42.768370",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "positive_reviews = df[df['Mode'] == 'Irrelevant']['Text']\n",
    "Irrelevant  = ' '.join(map(str, positive_reviews))\n",
    "pos_wordcloud = WordCloud(width=1500, height=800,\n",
    "                          background_color='black',\n",
    "                         stopwords=stopwords_list,\n",
    "                          min_font_size=15).generate(Irrelevant )\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(pos_wordcloud)\n",
    "plt.title('Word Cloud for Irrelevant Reviews')\n",
    "plt.axis('off')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "994fb29d",
   "metadata": {
    "papermill": {
     "duration": 0.055341,
     "end_time": "2023-11-03T11:17:46.054375",
     "exception": false,
     "start_time": "2023-11-03T11:17:45.999034",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "这里统计了一下数据集中的句子的长度的信息，如长度的最大值，最小值，均值，标准差等。我们将所有的句子处理为相同的长度166。（因为Mew + 2sigma是165.7）不足166的通过PAD填充，超过的就进行截断。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38327fef",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:46.166476Z",
     "iopub.status.busy": "2023-11-03T11:17:46.165745Z",
     "iopub.status.idle": "2023-11-03T11:17:46.234474Z",
     "shell.execute_reply": "2023-11-03T11:17:46.233368Z"
    },
    "papermill": {
     "duration": 0.125443,
     "end_time": "2023-11-03T11:17:46.236444",
     "exception": false,
     "start_time": "2023-11-03T11:17:46.111001",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "len_text = [len(tx) for tx in df['Text'].to_list()]\n",
    "print(f'Max Length : {np.max(len_text)}')\n",
    "print(f'Min Length : {np.min(len_text)}')\n",
    "print(f'Mean Length : {round(np.mean(len_text),2)}')\n",
    "print(f'Std Length : {round(np.std(len_text),2)}')\n",
    "print(f'Mew + 2sigma : {round(np.mean(len_text)+ 2 *np.std(len_text),2)}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb19fb88",
   "metadata": {
    "papermill": {
     "duration": 0.054517,
     "end_time": "2023-11-03T11:17:46.345933",
     "exception": false,
     "start_time": "2023-11-03T11:17:46.291416",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "这里使用柱状图来显示一下不同情感类型下最常用的单词。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff6c9c27",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:46.454626Z",
     "iopub.status.busy": "2023-11-03T11:17:46.453886Z",
     "iopub.status.idle": "2023-11-03T11:17:49.372580Z",
     "shell.execute_reply": "2023-11-03T11:17:49.371676Z"
    },
    "papermill": {
     "duration": 2.975907,
     "end_time": "2023-11-03T11:17:49.375540",
     "exception": false,
     "start_time": "2023-11-03T11:17:46.399633",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "for inx , value in enumerate(sentiments):\n",
    "    \n",
    "    counter = Counter(make_corpus(value))\n",
    "    most_common = counter.most_common()\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "\n",
    "    for word, count in most_common[:40]:\n",
    "         if word not in stop:\n",
    "            x.append(word)\n",
    "            y.append(count)\n",
    "         \n",
    "    sns.barplot(x=y, y=x, orient='h')\n",
    "    plt.title(f'{value} most used words')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d515e12b",
   "metadata": {
    "papermill": {
     "duration": 0.056548,
     "end_time": "2023-11-03T11:17:49.490916",
     "exception": false,
     "start_time": "2023-11-03T11:17:49.434368",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "为了让模型更加关注postive和negative的分类，这里将neutral和irrelevant统一赋值为2，然后positive和negative分别为1和0。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90fcbf3e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:49.612198Z",
     "iopub.status.busy": "2023-11-03T11:17:49.611374Z",
     "iopub.status.idle": "2023-11-03T11:17:49.667563Z",
     "shell.execute_reply": "2023-11-03T11:17:49.666834Z"
    },
    "papermill": {
     "duration": 0.118133,
     "end_time": "2023-11-03T11:17:49.669474",
     "exception": false,
     "start_time": "2023-11-03T11:17:49.551341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sentiments'] = df['Mode'].replace({'Positive' : 1 ,  'Negative' : 0 ,'Neutral':2 , 'Irrelevant' : 2 })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "337cce3f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:49.786492Z",
     "iopub.status.busy": "2023-11-03T11:17:49.786135Z",
     "iopub.status.idle": "2023-11-03T11:17:49.799599Z",
     "shell.execute_reply": "2023-11-03T11:17:49.798748Z"
    },
    "papermill": {
     "duration": 0.074073,
     "end_time": "2023-11-03T11:17:49.801414",
     "exception": false,
     "start_time": "2023-11-03T11:17:49.727341",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f08b725d",
   "metadata": {
    "papermill": {
     "duration": 0.057788,
     "end_time": "2023-11-03T11:17:49.919940",
     "exception": false,
     "start_time": "2023-11-03T11:17:49.862152",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "这里定义一个数据集的类\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce980a26",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:50.036055Z",
     "iopub.status.busy": "2023-11-03T11:17:50.035685Z",
     "iopub.status.idle": "2023-11-03T11:17:50.041938Z",
     "shell.execute_reply": "2023-11-03T11:17:50.041073Z"
    },
    "papermill": {
     "duration": 0.066736,
     "end_time": "2023-11-03T11:17:50.043862",
     "exception": false,
     "start_time": "2023-11-03T11:17:49.977126",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Dataset:\n",
    "    def __init__(self,text,sentiment):\n",
    "        self.text = text\n",
    "        self.sentiment = sentiment\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,item):\n",
    "        text = self.text[item,:]\n",
    "        target = self.sentiment[item]\n",
    "        return {\n",
    "            \"text\": torch.tensor(text,dtype = torch.long),\n",
    "            \"target\": torch.tensor(target,dtype = torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48eb95a6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:50.281558Z",
     "iopub.status.busy": "2023-11-03T11:17:50.281195Z",
     "iopub.status.idle": "2023-11-03T11:17:50.286710Z",
     "shell.execute_reply": "2023-11-03T11:17:50.285804Z"
    },
    "papermill": {
     "duration": 0.067515,
     "end_time": "2023-11-03T11:17:50.288570",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.221055",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_vectors(fname):\n",
    "    fin = open(fname , encoding=\"utf8\")\n",
    "    data = {}\n",
    "    for line in fin:\n",
    "        tokens = line.split()\n",
    "        data[tokens[0]] = np.array([float(value) for value in tokens[1:]])\n",
    "        \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc6b3d9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:50.405516Z",
     "iopub.status.busy": "2023-11-03T11:17:50.404715Z",
     "iopub.status.idle": "2023-11-03T11:17:50.410178Z",
     "shell.execute_reply": "2023-11-03T11:17:50.409327Z"
    },
    "papermill": {
     "duration": 0.065739,
     "end_time": "2023-11-03T11:17:50.412014",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.346275",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(word_index,embedding_dict):\n",
    "\n",
    "    embedding_matrix = np.zeros((len(word_index)+1,300))\n",
    "    for word, i in word_index.items():\n",
    "        if word in embedding_dict:\n",
    "            embedding_matrix[i] = embedding_dict[word]\n",
    "            \n",
    "    return embedding_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8577cf2f",
   "metadata": {
    "papermill": {
     "duration": 0.05667,
     "end_time": "2023-11-03T11:17:50.644088",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.587418",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<a id=\"1\"></a>\n",
    "# <div style=\"padding:20px;color:white;margin:0;font-size:30px;font-family:Georgia;text-align:center;display:fill;border-radius:5px;background-color:#254E58;overflow:hidden\"><b> 模型 </b></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b76400",
   "metadata": {
    "papermill": {
     "duration": 0.059455,
     "end_time": "2023-11-03T11:17:50.761648",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.702193",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<center>\n",
    "<img src=\"https://miro.medium.com/v2/resize:fit:764/1*6QnPUSv_t9BY9Fv8_aLb-Q.png\" width=800 height=300 />\n",
    "</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e51bfb",
   "metadata": {
    "papermill": {
     "duration": 0.056846,
     "end_time": "2023-11-03T11:17:50.876558",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.819712",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "双向LSTM（BiLSTM）是一种主要用于自然语言处理的递归神经网络。与标准LSTM不同，输入双向流动，并且能够利用来自两侧的信息。它可用于对单词和短语在序列的两个方向上的顺序依赖关系进行建模。\n",
    "\n",
    "BiLSTM又增加了一个LSTM层，这颠倒了信息流的方向。简单地说，这意味着输入序列在附加的LSTM层中反向流动。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1237351d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:50.994132Z",
     "iopub.status.busy": "2023-11-03T11:17:50.993463Z",
     "iopub.status.idle": "2023-11-03T11:17:51.002507Z",
     "shell.execute_reply": "2023-11-03T11:17:51.001603Z"
    },
    "papermill": {
     "duration": 0.070242,
     "end_time": "2023-11-03T11:17:51.004453",
     "exception": false,
     "start_time": "2023-11-03T11:17:50.934211",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class sentimentBiLSTM(nn.Module):\n",
    "#inherited from nn.Module\n",
    "    \n",
    "    def __init__(self, embedding_matrix, hidden_dim, output_size):\n",
    "      \n",
    "        #initializing the params by initialization method \n",
    "        super(sentimentBiLSTM, self).__init__()\n",
    "        self.embedding_matrix = embedding_matrix\n",
    "        self.hidden_dim = hidden_dim\n",
    "        num_words = self.embedding_matrix.shape[0]\n",
    "        embed_dim = self.embedding_matrix.shape[1]\n",
    "        # craetinh embedding layer\n",
    "        self.embedding = nn.Embedding(num_embeddings=num_words,embedding_dim=embed_dim)\n",
    "        \n",
    "        ## initializes the weights of the embedding layer to the pretrained embeddings in \n",
    "        ## embedding_matrix. It first converts embedding_matrix to a PyTorch tensor and \n",
    "        ## wraps it in an nn.Parameter object, which makes it a learnable parameter of the model.\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix,dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False\n",
    "        self.lstm = nn.LSTM(embed_dim,hidden_dim,bidirectional=True,batch_first=True)\n",
    "\n",
    "        #it is multuplied by 2 becuase it is bi_directional if one-sided it didnt need.\n",
    "        self.fc = nn.Linear(hidden_dim*2, output_size)\n",
    "        \n",
    "\n",
    "    #we need a forward function to model calculate the cost and know how bad the params is .  \n",
    "    # However , it can be written in a line of code but if we want to track it it is easier way.  \n",
    "    def forward(self, x):\n",
    "\n",
    "       \n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out,_ = self.lstm(embeds)\n",
    "        lstm_out = lstm_out[:, -1]\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aebe883",
   "metadata": {
    "papermill": {
     "duration": 0.060832,
     "end_time": "2023-11-03T11:17:51.125678",
     "exception": false,
     "start_time": "2023-11-03T11:17:51.064846",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "将数据集拆分为80%用于训练，20%用于测试。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2b7c45",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:51.243510Z",
     "iopub.status.busy": "2023-11-03T11:17:51.242633Z",
     "iopub.status.idle": "2023-11-03T11:17:51.283108Z",
     "shell.execute_reply": "2023-11-03T11:17:51.281956Z"
    },
    "papermill": {
     "duration": 0.101351,
     "end_time": "2023-11-03T11:17:51.285420",
     "exception": false,
     "start_time": "2023-11-03T11:17:51.184069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "y = df.sentiments.values\n",
    "train_df,test_df = train_test_split(df,test_size = 0.2, stratify = y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39500c39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:51.520298Z",
     "iopub.status.busy": "2023-11-03T11:17:51.519660Z",
     "iopub.status.idle": "2023-11-03T11:17:51.525760Z",
     "shell.execute_reply": "2023-11-03T11:17:51.524883Z"
    },
    "papermill": {
     "duration": 0.066563,
     "end_time": "2023-11-03T11:17:51.527787",
     "exception": false,
     "start_time": "2023-11-03T11:17:51.461224",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "MAX_LEN = 167\n",
    "BATCH_SIZE = 32\n",
    "hidden_dim = 64\n",
    "output_size = 3\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    \n",
    "       device = torch.device(\"cuda\")\n",
    "   \n",
    "else:\n",
    "       device = torch.device(\"cpu\")\n",
    "    \n",
    "\n",
    "print(f'Current device is {device}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291d73e6",
   "metadata": {
    "papermill": {
     "duration": 0.057981,
     "end_time": "2023-11-03T11:17:51.644422",
     "exception": false,
     "start_time": "2023-11-03T11:17:51.586441",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "现在，我们需要将每个样本转换为可读的方式，删除所有超过167的多余单词，并放入PAD字符<br>我们使用texts_to_sequences函数来完成这一操作。然后使用DataLoader来读取Train和Test的数据。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0f9c96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:51.760943Z",
     "iopub.status.busy": "2023-11-03T11:17:51.760575Z",
     "iopub.status.idle": "2023-11-03T11:17:54.862704Z",
     "shell.execute_reply": "2023-11-03T11:17:54.861892Z"
    },
    "papermill": {
     "duration": 3.163545,
     "end_time": "2023-11-03T11:17:54.865103",
     "exception": false,
     "start_time": "2023-11-03T11:17:51.701558",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(df.Text.values.tolist())\n",
    "\n",
    "xtrain = tokenizer.texts_to_sequences(train_df.Text.values)\n",
    "xtest = tokenizer.texts_to_sequences(test_df.Text.values)\n",
    "xtrain = tf.keras.preprocessing.sequence.pad_sequences(xtrain,maxlen = MAX_LEN)\n",
    "xtest = tf.keras.preprocessing.sequence.pad_sequences(xtest,maxlen = MAX_LEN)\n",
    "train_dataset = Dataset(text=xtrain,sentiment=train_df.sentiments.values)\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset,batch_size=BATCH_SIZE,drop_last=True)\n",
    "valid_dataset = Dataset(text=xtest,sentiment=test_df.sentiments.values)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_dataset,batch_size=BATCH_SIZE,drop_last=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb2edc7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:54.984371Z",
     "iopub.status.busy": "2023-11-03T11:17:54.983903Z",
     "iopub.status.idle": "2023-11-03T11:17:55.049222Z",
     "shell.execute_reply": "2023-11-03T11:17:55.048380Z"
    },
    "papermill": {
     "duration": 0.126573,
     "end_time": "2023-11-03T11:17:55.051130",
     "exception": false,
     "start_time": "2023-11-03T11:17:54.924557",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# check a batch of data \n",
    "one_batch = next(iter(train_loader))\n",
    "one_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d22cf",
   "metadata": {
    "papermill": {
     "duration": 0.057616,
     "end_time": "2023-11-03T11:17:55.167656",
     "exception": false,
     "start_time": "2023-11-03T11:17:55.110040",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "这里将每个数据中的word替换为word vector，使用的是glove.6B.300d的词向量。\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adde9d5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:17:55.283968Z",
     "iopub.status.busy": "2023-11-03T11:17:55.283617Z",
     "iopub.status.idle": "2023-11-03T11:18:38.176556Z",
     "shell.execute_reply": "2023-11-03T11:18:38.175701Z"
    },
    "papermill": {
     "duration": 42.953642,
     "end_time": "2023-11-03T11:18:38.178946",
     "exception": false,
     "start_time": "2023-11-03T11:17:55.225304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding_dict = load_vectors('./glove.6B.300d.txt')\n",
    "embedding_matrix = create_embedding_matrix(tokenizer.word_index,embedding_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca1d586d",
   "metadata": {
    "papermill": {
     "duration": 0.058462,
     "end_time": "2023-11-03T11:18:38.297299",
     "exception": false,
     "start_time": "2023-11-03T11:18:38.238837",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Description**<br>\n",
    "实例化模型。这里可以看到词典中共有23595个单词，每个单词的词向量是300维的。\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f2d38f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:18:38.412916Z",
     "iopub.status.busy": "2023-11-03T11:18:38.412578Z",
     "iopub.status.idle": "2023-11-03T11:18:45.400874Z",
     "shell.execute_reply": "2023-11-03T11:18:45.399734Z"
    },
    "papermill": {
     "duration": 7.048513,
     "end_time": "2023-11-03T11:18:45.403038",
     "exception": false,
     "start_time": "2023-11-03T11:18:38.354525",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# 实例化模型\n",
    "model = pass\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7893ce49",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:18:45.639237Z",
     "iopub.status.busy": "2023-11-03T11:18:45.638855Z",
     "iopub.status.idle": "2023-11-03T11:18:45.646629Z",
     "shell.execute_reply": "2023-11-03T11:18:45.645721Z"
    },
    "papermill": {
     "duration": 0.06827,
     "end_time": "2023-11-03T11:18:45.648682",
     "exception": false,
     "start_time": "2023-11-03T11:18:45.580412",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(42) \n",
    "torch.cuda.manual_seed(42)\n",
    "\n",
    "# 实例化优化器\n",
    "optimizer = pass\n",
    "\n",
    "criterion = nn.CrossEntropyLoss() \n",
    "\n",
    "def acc(pred,label):\n",
    "    pred = pred.argmax(1)\n",
    "    return torch.sum(pred == label.squeeze()).item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487f4e61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:18:45.766786Z",
     "iopub.status.busy": "2023-11-03T11:18:45.766425Z",
     "iopub.status.idle": "2023-11-03T11:20:11.780329Z",
     "shell.execute_reply": "2023-11-03T11:20:11.779044Z"
    },
    "papermill": {
     "duration": 86.075282,
     "end_time": "2023-11-03T11:20:11.782324",
     "exception": false,
     "start_time": "2023-11-03T11:18:45.707042",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "clip = 5\n",
    "epochs = 9\n",
    "valid_loss_min = np.Inf\n",
    "# train for some number of epochs\n",
    "epoch_tr_loss,epoch_vl_loss = [],[]\n",
    "epoch_tr_acc,epoch_vl_acc = [],[]\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # for getting loss and accuracy for train\n",
    "    train_losses = []\n",
    "    train_acc = 0.0\n",
    "\n",
    "    #put model on train mode\n",
    "    model.train()\n",
    "    correct = 0\n",
    "\n",
    "    # initialize hidden state \n",
    "    for data in train_loader:  \n",
    "\n",
    "        #get text and target \n",
    "        inputs = data['text']\n",
    "        labels = data['target']\n",
    "\n",
    "        #put them on GPU and right dtypes\n",
    "        inputs = inputs.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "\n",
    "        # 模型参数的梯度清零\n",
    "        pass\n",
    "        # 获取模型输出\n",
    "        pass\n",
    "        # 计算loss\n",
    "        pass\n",
    "        # loss反向传播\n",
    "        pass\n",
    "    \n",
    "        train_losses.append(loss.item())\n",
    "        # accuracy\n",
    "        accuracy = acc(output,labels)\n",
    "        train_acc += accuracy\n",
    "        #`clip_grad_norm` helps prevent the exploding gradient problem in LSTMs\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        \n",
    "    # for getting loss and accuracy for valiadtion\n",
    "    val_losses = []\n",
    "    val_acc = 0.0\n",
    "\n",
    "    #put model on evaluation mode\n",
    "    model.eval()\n",
    "    for data in valid_loader:\n",
    "\n",
    "        #get text and target \n",
    "        inputs = data['text']\n",
    "        labels = data['target']\n",
    "\n",
    "        #put them on GPU and right dtypes\n",
    "        inputs = inputs.to(device,dtype=torch.long)\n",
    "        labels = labels.to(device,dtype=torch.float)\n",
    "\n",
    "        #gradient becomes zero=> avoid accumulating \n",
    "        model.zero_grad()\n",
    "        output = model(inputs)\n",
    "\n",
    "        output = model(inputs)\n",
    "        #Loss calculating \n",
    "        val_loss = criterion(output, labels.long())\n",
    "        #append Loss to the above list\n",
    "        val_losses.append(val_loss.item())\n",
    "\n",
    "        # calculating accuracy \n",
    "        accuracy = acc(output,labels)\n",
    "        val_acc += accuracy\n",
    "        epoch_train_loss = np.mean(train_losses)\n",
    "\n",
    "        #using schedule lr if you need\n",
    "#         schedul_learning.step()\n",
    "#         schedul_learning\n",
    "\n",
    "    #appending all accuracy and loss to the above lists and variables\n",
    "    epoch_val_loss = np.mean(val_losses)\n",
    "    epoch_train_acc = train_acc/len(train_loader.dataset)\n",
    "    epoch_val_acc = val_acc/len(valid_loader.dataset)\n",
    "    epoch_tr_loss.append(epoch_train_loss)\n",
    "    epoch_vl_loss.append(epoch_val_loss)\n",
    "    epoch_tr_acc.append(epoch_train_acc)\n",
    "    epoch_vl_acc.append(epoch_val_acc)\n",
    "    print(f'Epoch {epoch+1}') \n",
    "    print(f'train_loss : {epoch_train_loss} val_loss : {epoch_val_loss}')\n",
    "    print(f'train_accuracy : {epoch_train_acc*100} val_accuracy : {epoch_val_acc*100}')\n",
    "    if epoch_val_loss <= valid_loss_min:\n",
    "        #each time that model(params) get better you can save the model(you have to enter a path ou you pc and save with pt file)\n",
    "        # torch.save(model.state_dict(), r'C:\\Users\\payama\\Desktop\\Projects kaggle\\NLP\\vectors features\\BidirectionalLSTM.pt')\n",
    "#         print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(valid_loss_min,epoch_val_loss))\n",
    "        print(f'Validation loss decreased ({valid_loss_min} --> {epoch_val_loss})  Saving model ...')\n",
    "        # save model if better result happends\n",
    "        valid_loss_min = epoch_val_loss\n",
    "    print(30 * '==' , '>')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a80da8e",
   "metadata": {
    "papermill": {
     "duration": 0.05878,
     "end_time": "2023-11-03T11:20:11.901901",
     "exception": false,
     "start_time": "2023-11-03T11:20:11.843121",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "<div style=\"font-family:Georgia;background-color:aliceblue; padding:30px; font-size:17px ; color:#034914\">\n",
    "\n",
    "**Descripton**<br>\n",
    "    \n",
    "The result after 10 epochs is :<br>\n",
    "    \n",
    "train_accuracy : 92.44121135998884 <br>\n",
    "val_accuracy : 82.91117158607216<br>\n",
    "    \n",
    "train_loss : 0.18834395279212413<br>\n",
    "val_loss : 0.5528476672784594<br>  \n",
    "\n",
    " \n",
    "Lets plot it for better understanding =>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de12bcf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:20:12.021489Z",
     "iopub.status.busy": "2023-11-03T11:20:12.020861Z",
     "iopub.status.idle": "2023-11-03T11:20:12.440593Z",
     "shell.execute_reply": "2023-11-03T11:20:12.439694Z"
    },
    "papermill": {
     "duration": 0.482308,
     "end_time": "2023-11-03T11:20:12.442556",
     "exception": false,
     "start_time": "2023-11-03T11:20:11.960248",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,10),epoch_tr_acc , label='train accuracy')\n",
    "plt.scatter(range(1,10),epoch_tr_acc)\n",
    "plt.plot(range(1,10),epoch_vl_acc , label='val accuracy')\n",
    "plt.scatter(range(1,10),epoch_vl_acc)\n",
    "plt.title('Accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5c38bf1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-11-03T11:20:12.567787Z",
     "iopub.status.busy": "2023-11-03T11:20:12.567101Z",
     "iopub.status.idle": "2023-11-03T11:20:12.994230Z",
     "shell.execute_reply": "2023-11-03T11:20:12.993326Z"
    },
    "papermill": {
     "duration": 0.491808,
     "end_time": "2023-11-03T11:20:12.996165",
     "exception": false,
     "start_time": "2023-11-03T11:20:12.504357",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(7,5))\n",
    "plt.plot(range(1,10),epoch_tr_loss , label='train loss')\n",
    "plt.scatter(range(1,10),epoch_tr_loss )\n",
    "plt.plot(range(1,10),epoch_vl_loss , label='val loss')\n",
    "plt.scatter(range(1,10),epoch_vl_loss)\n",
    "plt.title('Loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 230.678201,
   "end_time": "2023-11-03T11:20:16.076374",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-11-03T11:16:25.398173",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
